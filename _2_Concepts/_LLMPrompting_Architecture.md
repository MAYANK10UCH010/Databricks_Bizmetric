# Key LLM Prompting & Architecture Concepts

| Concept               | Definition                                                                 | Invention / Origin                                                   | Reference                                                                                      |
|------------------------|-----------------------------------------------------------------------------|-----------------------------------------------------------------------|------------------------------------------------------------------------------------------------|
| **Zero-Shot Learning** | Model answers without task-specific examples, relying on pretraining.      | Popularized by GPT-3 (Brown et al., 2020).                           | [GPT-3 Paper (Brown et al., 2020)](https://arxiv.org/abs/2005.14165)                          |
| **Few-Shot Learning**  | Providing a handful of input-output examples in the prompt to guide model. | GPT-3 demonstrations showed strong few-shot performance (2020).      | [GPT-3 Paper](https://arxiv.org/abs/2005.14165)                                               |
| **Chain-of-Thought (CoT)** | Asking models to generate step-by-step reasoning before final answer.       | Wei et al., 2022 (“Chain of Thought Prompting Elicits Reasoning”).   | [Wei et al., 2022](https://arxiv.org/abs/2201.11903)                                          |
| **Self-Consistency**   | Sampling multiple reasoning paths (CoT) and aggregating for robust answers.| Introduced by Wang et al., 2022.                                     | [Wang et al., 2022](https://arxiv.org/abs/2203.11171)                                         |
| **Tree-of-Thought (ToT)** | Extends CoT: explores multiple reasoning branches like a search tree.       | Yao et al., 2023.                                                     | [Yao et al., 2023](https://arxiv.org/abs/2305.10601)                                          |
| **ReAct**              | Combines *Reasoning + Acting*: model alternates between thoughts & actions.| Yao et al., 2022.                                                     | [ReAct Paper](https://arxiv.org/abs/2210.03629)                                               |
| **Reflexion**          | Agent improves iteratively by reflecting on past errors.                   | Shinn et al., 2023.                                                   | [Reflexion Paper](https://arxiv.org/abs/2303.11366)                                           |
| **Mixture of Experts (MoE)** | Uses multiple expert subnetworks; router activates subset per input.         | First: Jacobs et al., 1991; Scaled: Google Switch Transformer (2021).| [Switch Transformer](https://arxiv.org/abs/2101.03961)                                        |
| **Instruction Tuning** | Fine-tuning models on curated instruction datasets for better following.   | Introduced with FLAN (Wei et al., 2021).                             | [FLAN Paper](https://arxiv.org/abs/2109.01652)                                                |
| **RLHF**               | Aligning LLMs with human preferences using Reinforcement Learning.         | InstructGPT (Ouyang et al., 2022).                                   | [InstructGPT](https://arxiv.org/abs/2203.02155)                                               |
| **Constitutional AI**  | Replacing human feedback with AI-written “constitutional” rules.           | Anthropic, 2022.                                                     | [Anthropic Blog](https://www.anthropic.com/index/introducing-constitutional-ai)               |
| **RAG (Retrieval-Augmented Generation)** | Enhancing model with retrieved external knowledge before generation. | Lewis et al., 2020.                                                   | [RAG Paper](https://arxiv.org/abs/2005.11401)                                                 |
| **MemPrompt**          | Prompting that incorporates memory of past conversations.                  | Xie et al., 2023.                                                     | [MemPrompt](https://arxiv.org/abs/2305.00981)                                                 |
| **LoRA (Low-Rank Adaptation)** | Efficient fine-tuning by injecting low-rank updates into weights.        | Hu et al., 2021.                                                      | [LoRA Paper](https://arxiv.org/abs/2106.09685)                                                |
| **PEFT (Parameter Efficient Fine-Tuning)** | Family of lightweight tuning methods (LoRA, adapters, prefix-tuning). | Many works, consolidated in 2021–22.                                | [Hugging Face PEFT](https://huggingface.co/docs/peft/index)                                   |
| **Prompt Engineering** | Crafting input prompts to steer model behavior effectively.                | Emerged with GPT-3 usage (2020+).                                    | [OpenAI Prompt Guide](https://platform.openai.com/docs/guides/prompt-engineering)              |
| **Meta-Prompting**     | Higher-level prompt templates guiding model outputs across tasks.          | Adopted in enterprise RAG setups (2022+).                            | [Databricks Cookbook](https://docs.databricks.com/en/generative-ai/tutorials/ai-cookbook)     |
| **Guardrails**         | Post-processing or rules enforcing safety, format, or consistency.         | Evolved from safety research (2021+).                                | [Guardrails AI](https://shreyar.github.io/guardrails/)                                        |
| **AutoPrompt**         | Automated search for optimal prompts using gradient-based methods.         | Shin et al., 2020.                                                    | [AutoPrompt Paper](https://arxiv.org/abs/2010.15980)                                          |
| **Direct Preference Optimization (DPO)** | Trains LLMs directly on preference pairs without RL.                 | Rafailov et al., 2023.                                                | [DPO Paper](https://arxiv.org/abs/2305.18290)                                                 |
| **Speculative Decoding** | Speed-up inference by drafting with small model, verifying with large one. | Chen et al., 2023 (OpenAI).                                           | [Speculative Decoding](https://arxiv.org/abs/2302.01318)                                      |
