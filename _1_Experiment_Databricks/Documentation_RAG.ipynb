{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c0d8d1a-2ffa-40a0-85ef-1c07884538a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ðŸ“‚ Option 1: Download to /tmp/ (local storage)\n",
    "### 1. Download sample PDFs from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35134d22-f8bb-4693-bf63-675323724deb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "mkdir -p /tmp/pdfs\n",
    "wget -O /tmp/pdfs/sample1.pdf https://github.com/mozilla/pdf.js-sample-files/blob/master/tracemonkey.pdf?raw=true\n",
    "#wget -O /tmp/pdfs/sample2.pdf tracemonkey.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bef20e53-32ea-40e4-a2b2-c47aaa65288b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Verify Files Exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ece890f9-436c-4df9-9278-dc91a92108fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir(\"/tmp/pdfs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d516563-3054-4798-b88f-59f3d2c45c1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "ls -lh /tmp/pdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77fa95e3-a337-4620-a50f-9c8a77a7c267",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Note: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98c75f95-a9b4-47cf-837a-e94c559f4369",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --quiet PyPDF2 sentence-transformers faiss-cpu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75cfd42f-c23c-41de-80a3-8be14c45673b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "593a91b7-3c0c-40d9-ad14-11eca262ec04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####âš¡ Rule of Thumb:\n",
    "If Spark canâ€™t guess the schema â†’ you must tell it explicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bcc3ba84-2705-4bd2-a12c-9b5b1faa21f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Re-create the page-level DataFrame (your working patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2368301-18d8-43d5-80fb-29041a86d611",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import PyPDF2\n",
    "# from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# pdf_path = \"/tmp/pdfs/sample1.pdf\"\n",
    "\n",
    "# reader = PyPDF2.PdfReader(pdf_path)\n",
    "\n",
    "# # Collect page-level chunks\n",
    "# page_chunks = []\n",
    "# for i, page in enumerate(reader.pages):\n",
    "#     text = page.extract_text()\n",
    "#     if text:\n",
    "#         page_chunks.append((f\"sample1.pdf\", f\"page_{i+1}\", text))\n",
    "\n",
    "# # Schema for DataFrame\n",
    "# schema = StructType([\n",
    "#     StructField(\"filename\", StringType(), True),\n",
    "#     StructField(\"page\", StringType(), True),\n",
    "#     StructField(\"content\", StringType(), True)\n",
    "# ])\n",
    "\n",
    "# # Create Spark DataFrame\n",
    "# page_df = spark.createDataFrame(page_chunks, schema=schema)\n",
    "# display(page_df.limit(5))\n",
    "\n",
    "\n",
    "# %python\n",
    "import os\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import PyPDF2\n",
    "\n",
    "pdf_path = \"/tmp/pdfs/sample1.pdf\"   # <-- change if needed\n",
    "reader = PyPDF2.PdfReader(pdf_path)\n",
    "\n",
    "# Collect page-level chunks\n",
    "page_chunks = []\n",
    "for i, page in enumerate(reader.pages):\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        page_chunks.append((os.path.basename(pdf_path), f\"page_{i+1}\", text))\n",
    "\n",
    "# Schema for DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"filename\", StringType(), True),\n",
    "    StructField(\"page\", StringType(), True),\n",
    "    StructField(\"content\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create Spark DataFrame\n",
    "page_df = spark.createDataFrame(page_chunks, schema=schema)\n",
    "display(page_df.limit(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2406e33-9ead-4905-acf3-2b791fa85709",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Persist page-level DataFrame into a Delta (Bronze) table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5901b930-4bd2-4473-866e-b333f461de3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %python\n",
    "# Create a small database to keep things organized (optional)\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS demo_docs\")\n",
    "\n",
    "# Write bronze\n",
    "page_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"demo_docs.pages_bronze\")\n",
    "print(\"Saved demo_docs.pages_bronze\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1c2e0d7-063d-4f25-91fa-8f03cf255729",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Chunk text strings into smaller pieces (sliding window / overlap)\n",
    "We'll chunk by approximate word count. You can change chunk_size and overlap to tune retrieval granularity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02ac0392-1588-4e1c-85cd-0ca16eb109ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> [!NOTE]: What is **RDD FlatMap**</br>\n",
    "Apache Spark Map vs FlatMap Operation - DataFlairAn RDD flatMap in Apache Spark is a transformation operation that applies a function to each element of a Resilient Distributed Dataset (RDD) and then flattens the results into a single RDD, effectively performing a one-to-many mapping. Unlike map, which returns a single output for each input, flatMap can return zero, one, or more elements from the function, making it ideal for scenarios like splitting a line of text into individual word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac7a0821-b342-4427-aa7b-f757e5019e6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6154932c-fb2b-4484-9e5a-30060b41b1a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. Create embeddings for each chunk (using Sentence-Transformers)\n",
    "\n",
    "For Free Edition weâ€™ll use an in-session model (all-MiniLM-L6-v2) from sentence-transformers. This is small and fast. We will convert the chunk DataFrame to pandas (for small datasets) and compute embeddings in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f13a5b8e-79fd-449e-a16b-1b5c5f2acbdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8050357015040461,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Documentation_RAG",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
