{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c0d8d1a-2ffa-40a0-85ef-1c07884538a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ðŸ“‚ Option 1: Download to /tmp/ (local storage)\n",
    "### 1. Download sample PDFs from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35134d22-f8bb-4693-bf63-675323724deb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "mkdir -p /tmp/pdfs\n",
    "wget -O /tmp/pdfs/sample1.pdf https://github.com/mozilla/pdf.js-sample-files/blob/master/tracemonkey.pdf?raw=true\n",
    "#wget -O /tmp/pdfs/sample2.pdf tracemonkey.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bef20e53-32ea-40e4-a2b2-c47aaa65288b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Verify Files Exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ece890f9-436c-4df9-9278-dc91a92108fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir(\"/tmp/pdfs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d516563-3054-4798-b88f-59f3d2c45c1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "ls -lh /tmp/pdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77fa95e3-a337-4620-a50f-9c8a77a7c267",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Note: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98c75f95-a9b4-47cf-837a-e94c559f4369",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --quiet PyPDF2 sentence-transformers faiss-cpu pandas databricks-vectorsearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75cfd42f-c23c-41de-80a3-8be14c45673b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "593a91b7-3c0c-40d9-ad14-11eca262ec04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####âš¡ Rule of Thumb:\n",
    "If Spark canâ€™t guess the schema â†’ you must tell it explicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcc3ba84-2705-4bd2-a12c-9b5b1faa21f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Re-create the page-level DataFrame (your working patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2368301-18d8-43d5-80fb-29041a86d611",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %python\n",
    "import os\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
    "import PyPDF2\n",
    "\n",
    "pdf_path = \"/tmp/pdfs/sample1.pdf\"   # <-- change if needed\n",
    "reader = PyPDF2.PdfReader(pdf_path)\n",
    "\n",
    "# Collect page-level chunks\n",
    "page_chunks = []\n",
    "for i, page in enumerate(reader.pages):\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        page_chunks.append((os.path.basename(pdf_path), f\"page_{i+1}\", text))\n",
    "\n",
    "# Schema for DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"filename\", StringType(), True),\n",
    "    StructField(\"page\", StringType(), True),\n",
    "    StructField(\"content\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create Spark DataFrame\n",
    "page_df = spark.createDataFrame(page_chunks, schema=schema)\n",
    "display(page_df.limit(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2406e33-9ead-4905-acf3-2b791fa85709",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Persist page-level DataFrame into a Delta (Bronze) table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5901b930-4bd2-4473-866e-b333f461de3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %python\n",
    "# Create a small database to keep things organized (optional)\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS demo_docs\")\n",
    "\n",
    "# Write bronze\n",
    "page_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"demo_docs.pages_bronze\")\n",
    "print(\"Saved demo_docs.pages_bronze\")\n",
    "\n",
    "#spark.sql(\"DESCRIBE DETAIL demo_docs\").select(\"location\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1c2e0d7-063d-4f25-91fa-8f03cf255729",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Chunk text strings into smaller pieces (sliding window / overlap)\n",
    "We'll chunk by approximate word count. You can change chunk_size and overlap to tune retrieval granularity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02ac0392-1588-4e1c-85cd-0ca16eb109ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> [!NOTE]: What is **RDD FlatMap**</br>\n",
    "Apache Spark Map vs FlatMap Operation - DataFlairAn RDD flatMap in Apache Spark is a transformation operation that applies a function to each element of a Resilient Distributed Dataset (RDD) and then flattens the results into a single RDD, effectively performing a one-to-many mapping. Unlike map, which returns a single output for each input, flatMap can return zero, one, or more elements from the function, making it ideal for scenarios like splitting a line of text into individual word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17b50676-abe8-42ed-865c-7a4c0e1513be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %python\n",
    "from pyspark.sql import Row\n",
    "import math \n",
    "import pandas as pd\n",
    "\n",
    "# 1. Collect Spark DF -> Pandas\n",
    "page_pdf = page_df.toPandas()\n",
    "\n",
    "# 2. Define chunking function (same as before)\n",
    "def chunk_text_words(text, chunk_size=200, overlap=50):\n",
    "    if not text:\n",
    "        return []\n",
    "    words = text.split()\n",
    "    n = len(words)\n",
    "    if n <= chunk_size:\n",
    "        return [\" \".join(words)]\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < n:\n",
    "        end = start + chunk_size\n",
    "        chunk = \" \".join(words[start:min(end, n)])\n",
    "        chunks.append(chunk)\n",
    "        if end >= n:\n",
    "            break\n",
    "        start = end - overlap\n",
    "    return chunks\n",
    "\n",
    "# 3. Apply chunking\n",
    "chunk_records = []\n",
    "for _, row in page_pdf.iterrows():\n",
    "    chunks = chunk_text_words(row[\"content\"], chunk_size=200, overlap=50)\n",
    "    for idx, c in enumerate(chunks):\n",
    "        chunk_records.append({\n",
    "            \"filename\": row[\"filename\"],\n",
    "            \"page\": row[\"page\"],\n",
    "            \"chunk_id\": idx+1,\n",
    "            \"content\": c\n",
    "        })\n",
    "\n",
    "# 4. Convert back to Spark\n",
    "chunks_df = spark.createDataFrame(chunk_records)\n",
    "\n",
    "# 5. Save into Delta\n",
    "chunks_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"demo_docs.chunks_bronze\")\n",
    "\n",
    "display(chunks_df.limit(10))\n",
    "\n",
    "\n",
    "\n",
    "# # Chunking function (word-based, sliding-window)\n",
    "# def chunk_text_words(text, chunk_size=200, overlap=50):\n",
    "#     \"\"\"\n",
    "#     chunk_size = number of words per chunk\n",
    "#     overlap = number of words to overlap between adjacent chunks\n",
    "#     \"\"\"\n",
    "#     if not text:\n",
    "#         return []\n",
    "#     words = text.split()\n",
    "#     n = len(words)\n",
    "#     if n <= chunk_size:\n",
    "#         return [\" \".join(words)]\n",
    "#     chunks = []\n",
    "#     start = 0\n",
    "#     while start < n:\n",
    "#         end = start + chunk_size\n",
    "#         chunk = \" \".join(words[start:min(end, n)])\n",
    "#         chunks.append(chunk)\n",
    "#         if end >= n:\n",
    "#             break\n",
    "#         start = end - overlap\n",
    "#     return chunks\n",
    "\n",
    "\n",
    "# # Schema for the output\n",
    "# chunk_schema = StructType([\n",
    "#     StructField(\"filename\", StringType(), True),\n",
    "#     StructField(\"page\", StringType(), True),\n",
    "#     StructField(\"chunk_id\", IntegerType(), True),\n",
    "#     StructField(\"content\", StringType(), True)\n",
    "# ])\n",
    "\n",
    "# # Function to apply per batch (pandas df -> pandas df)\n",
    "# def chunk_pages(pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "#     records = []\n",
    "#     for _, row in pdf.iterrows():\n",
    "#         chunks = chunk_text_words(row[\"content\"], chunk_size=200, overlap=50)\n",
    "#         for idx, c in enumerate(chunks):\n",
    "#             records.append({\n",
    "#                 \"filename\": row[\"filename\"],\n",
    "#                 \"page\": row[\"page\"],\n",
    "#                 \"chunk_id\": idx + 1,\n",
    "#                 \"content\": c\n",
    "#             })\n",
    "#     return pd.DataFrame(records)\n",
    "\n",
    "# # Apply mapInPandas\n",
    "# chunks_df = page_df.mapInPandas(chunk_pages, schema=chunk_schema)\n",
    "\n",
    "# # Save to Delta\n",
    "# chunks_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"demo_docs.chunks_bronze\")\n",
    "\n",
    "# display(chunks_df.limit(10))\n",
    "\n",
    "# # Convert page_df -> chunk rows using RDD flatMap\n",
    "# # def page_to_chunks(row):\n",
    "# #     # row: (filename, page, content)\n",
    "# #     chunks = chunk_text_words(row['content'], chunk_size=200, overlap=50)\n",
    "# #     out = []\n",
    "# #     for idx, c in enumerate(chunks):\n",
    "# #         out.append(Row(filename=row['filename'],\n",
    "# #                        page=row['page'],\n",
    "# #                        chunk_id=idx+1,\n",
    "# #                        content=c))\n",
    "# #     return out\n",
    "\n",
    "# # rdd_chunks = page_df.rdd.flatMap(page_to_chunks)\n",
    "# # chunks_df = spark.createDataFrame(rdd_chunks)\n",
    "\n",
    "# # # Persist chunk table (bronze -> silver in a real pipeline)\n",
    "# # chunks_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"demo_docs.chunks_bronze\")\n",
    "# # print(\"Saved demo_docs.chunks_bronze\")\n",
    "# # display(chunks_df.limit(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6154932c-fb2b-4484-9e5a-30060b41b1a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. Create embeddings for each chunk (using Sentence-Transformers)\n",
    "\n",
    "For Free Edition weâ€™ll use an in-session model (all-MiniLM-L6-v2) from sentence-transformers. This is small and fast. We will convert the chunk DataFrame to pandas (for small datasets) and compute embeddings in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f13a5b8e-79fd-449e-a16b-1b5c5f2acbdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Load small embedding model (works locally)\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Pull chunks into pandas (ONLY for modest data sizes).\n",
    "# For larger datasets, use mapInPandas or handle in streaming/batches.\n",
    "chunks_pd = chunks_df.select(\"filename\",\"page\",\"chunk_id\",\"content\").toPandas()\n",
    "print(\"Number of chunks:\", len(chunks_pd))\n",
    "\n",
    "# Compute embeddings in batches to avoid OOM\n",
    "batch_size = 64\n",
    "embeddings = []\n",
    "for i in range(0, len(chunks_pd), batch_size):\n",
    "    batch_texts = chunks_pd[\"content\"].iloc[i:i+batch_size].tolist()\n",
    "    emb_batch = model.encode(batch_texts, show_progress_bar=False, convert_to_numpy=True)\n",
    "    embeddings.append(emb_batch)\n",
    "embeddings = np.vstack(embeddings)  # shape: (num_chunks, dim)\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n",
    "\n",
    "# Attach embeddings and persistent id\n",
    "chunks_pd[\"vector_id\"] = np.arange(len(chunks_pd))  # unique int id on the vector namespace\n",
    "chunks_pd[\"embedding\"] = embeddings.tolist()\n",
    "\n",
    "# Convert back to Spark DataFrame (embedding column will be an ArrayType(DoubleType()))\n",
    "from pyspark.sql import SparkSession\n",
    "emb_spark = spark.createDataFrame(chunks_pd)\n",
    "# Save as embeddings table\n",
    "emb_spark.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"demo_docs.chunks_with_embeddings\")\n",
    "print(\"Saved demo_docs.chunks_with_embeddings\")\n",
    "display(emb_spark.limit(5))\n",
    "# spark.sql(\"DESCRIBE DETAIL demo_docs\").select(\"location\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0ab3fd1-bb73-487d-848d-08dba42553e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5. Build a local FAISS index (for similarity search in Free Edition)\n",
    "We save the FAISS index file to /tmp/ and keep a mapping table (vector_id â†’ chunk metadata)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6855088d-f013-4ded-ab1e-95b2ea113222",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %python\n",
    "import faiss\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Fetch embeddings numpy array (assume small-ish)\n",
    "vecs = embeddings.astype(np.float32)\n",
    "d = vecs.shape[1]\n",
    "index = faiss.IndexFlatL2(d)   # exact index (Vector Search Index); for large data use IndexIVFFlat or HNSW\n",
    "index.add(vecs)\n",
    "print(\"FAISS index size:\", index.ntotal)\n",
    "\n",
    "# Save index to /tmp (ephemeral)\n",
    "faiss_file = \"/tmp/faiss_chunks.index\"\n",
    "faiss.write_index(index, faiss_file)\n",
    "print(\"FAISS index saved to\", faiss_file)\n",
    "\n",
    "# Save mapping (vector_id -> filename,page,chunk_id) as spark table if not already\n",
    "map_pd = chunks_pd[[\"vector_id\",\"filename\",\"page\",\"chunk_id\",\"content\"]]\n",
    "map_spark = spark.createDataFrame(map_pd)\n",
    "map_spark.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"demo_docs.vectorid_to_chunk\")\n",
    "print(\"Saved demo_docs.vectorid_to_chunk\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c87bab7-90d5-4ab0-8aaa-923dd3de3cfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 6. Example: Run similarity search (query â†’ embeddings â†’ FAISS lookup â†’ get chunk content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b680afdf-9eb8-4e3d-8e1b-0ca33b8c7f0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %python\n",
    "# Query text\n",
    "query_text = \"Explain Trace Trees\"   # replace with user question\n",
    "\n",
    "# Create embedding for query\n",
    "q_emb = model.encode([query_text]).astype(np.float32)\n",
    "\n",
    "# Load index from file (just to demonstrate persist/load)\n",
    "index = faiss.read_index(\"/tmp/faiss_chunks.index\")\n",
    "\n",
    "# Search top_k\n",
    "top_k = 5\n",
    "D, I = index.search(q_emb, top_k)   # I is indices into the vectors array\n",
    "print(\"Distances:\", D)\n",
    "print(\"Indices:\", I)\n",
    "\n",
    "# I is vector index positions which we set equal to vector_id when creating\n",
    "vector_ids = I[0].tolist()\n",
    "\n",
    "# Fetch metadata + content for matched vector_ids from spark table\n",
    "vector_ids_str = \",\".join(str(int(v)) for v in vector_ids)\n",
    "sql = f\"SELECT * FROM demo_docs.vectorid_to_chunk WHERE vector_id IN ({vector_ids_str})\"\n",
    "hits = spark.sql(sql).toPandas()\n",
    "print(hits[[\"vector_id\",\"filename\",\"page\",\"chunk_id\",\"content\"]].to_dict(orient=\"records\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a33d790-b637-474d-9671-55751067760b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7. (Optional) Persist the FAISS index file outside the cluster\n",
    "\n",
    "Because /tmp is **ephemeral**, you may want to download the index to your laptop, or upload to a GitHub release / cloud storage via curl/aws s3 cp etc. For Free Edition without cloud credentials, download via browser: expose the file via a tiny HTTP server (not recommended for security) or copy file content and save locally. Simpler: re-create indexing steps whenever you restart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c5f710e-9b0b-4c3e-b319-026b718e058f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## âœ… Good Practices & Notes\n",
    "\n",
    "- **Chunk size / overlap**  \n",
    "  Typical chunk sizes: **150â€“500 words** with overlap **20â€“100 words**.  \n",
    "  Tune these depending on your LLMâ€™s context window and retrieval quality.\n",
    "\n",
    "- **Batch embeddings**  \n",
    "  Generate embeddings in batches to avoid memory issues.  \n",
    "  Example: `batch_size = 64`.\n",
    "\n",
    "- **Large data**  \n",
    "  For large corpora:  \n",
    "  - Compute embeddings in worker nodes (e.g., `mapInPandas` or distributed UDFs).  \n",
    "  - Write embeddings incrementally to Delta for scalability.\n",
    "\n",
    "- **Vector index type**  \n",
    "  - For small demos: use **`IndexFlatL2`** (exact search).  \n",
    "  - For larger corpora: use **IVF** or **HNSW** indexes to reduce memory usage and improve retrieval speed.\n",
    "\n",
    "- **Storage**  \n",
    "  Save embeddings and mapping as **Delta tables** so they can be queried with SQL.  \n",
    "  This ensures a **single source of truth**.\n",
    "\n",
    "- **Free Edition constraints**  \n",
    "  - Everything in `/tmp` is **ephemeral** â†’ re-run steps after cluster restarts.  \n",
    "  - **Databricks Vector Search** (managed indexes) and **Unity Catalog** may not be available.  \n",
    "    â†’ Use **FAISS** as a local alternative.\n",
    "\n",
    "- **Downstream usage**  \n",
    "  After retrieving candidate chunks:  \n",
    "  - Pass top-k chunks (or a concatenated/summary version) into your **LLM / QA model** (RAG pipeline).  \n",
    "  - For higher precision, consider **reranking** using a cross-encoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "274b28df-f639-43d0-bd71-a7038b58413a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "\n",
    "# ---- Structured return type ----\n",
    "@dataclass\n",
    "class SearchResult:\n",
    "    id: str\n",
    "    score: float\n",
    "    content: str\n",
    "    metadata: Optional[dict] = None\n",
    "\n",
    "# ---- Search wrapper ----\n",
    "def run_search(\n",
    "    client: VectorSearchClient,\n",
    "    index_name: str,\n",
    "    query: str,\n",
    "    k: int = 5\n",
    ") -> List[SearchResult]:\n",
    "    \"\"\"\n",
    "    Run semantic vector search against a Databricks Vector Search Index.\n",
    "\n",
    "    Args:\n",
    "        client: Databricks VectorSearchClient instance.\n",
    "        index_name: Name of the vector search index.\n",
    "        query: Natural language query string.\n",
    "        k: Top-k results to return.\n",
    "\n",
    "    Returns:\n",
    "        List of SearchResult objects with id, score, content, and metadata.\n",
    "    \"\"\"\n",
    "    index = client.get_index(index_name)\n",
    "\n",
    "    results = index.similarity_search(\n",
    "        query_vector=query,\n",
    "        k=k,\n",
    "        return_metadata=True\n",
    "    )\n",
    "\n",
    "    return [\n",
    "        SearchResult(\n",
    "            id=row[\"id\"],\n",
    "            score=row[\"score\"],\n",
    "            content=row[\"metadata\"].get(\"content\", \"\"),\n",
    "            metadata=row[\"metadata\"]\n",
    "        )\n",
    "        for row in results[\"matches\"]\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcea0d57-d8c2-4e24-bc87-aa7f788e845b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# After FAISS search\n",
    "vector_ids = I[0].tolist()\n",
    "\n",
    "# # Fetch metadata and chunk content\n",
    "# vector_ids_str = \",\".join(str(int(v)) for v in vector_ids)\n",
    "\n",
    "# sql = f\"\"\"\n",
    "# SELECT vector_id, filename, page, chunk_id, content\n",
    "# FROM demo_docs.vectorid_to_chunk\n",
    "# WHERE vector_id IN ({vector_ids_str})\n",
    "# ORDER BY FIELD(vector_id, {vector_ids_str})  -- preserve FAISS ranking\n",
    "# \"\"\"\n",
    "# hits = spark.sql(sql).toPandas()\n",
    "\n",
    "order_expr = \"CASE\"\n",
    "for rank, vid in enumerate(vector_ids):\n",
    "    order_expr += f\" WHEN vector_id = {vid} THEN {rank}\"\n",
    "order_expr += \" END\"\n",
    "\n",
    "sql = f\"\"\"\n",
    "SELECT vector_id, filename, page, chunk_id, content\n",
    "FROM demo_docs.vectorid_to_chunk\n",
    "WHERE vector_id IN ({vector_ids_str})\n",
    "ORDER BY {order_expr}\n",
    "\"\"\"\n",
    "hits = spark.sql(sql).toPandas()\n",
    "\n",
    "#############################################################\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class SearchHit:\n",
    "    filename: str\n",
    "    page: int\n",
    "    chunk_id: int\n",
    "    content: str\n",
    "\n",
    "# Convert DataFrame to list of SearchHit objects\n",
    "hit_objects = [\n",
    "    SearchHit(\n",
    "        filename=row[\"filename\"],\n",
    "        page=row[\"page\"],\n",
    "        chunk_id=row[\"chunk_id\"],\n",
    "        content=row[\"content\"]\n",
    "    )\n",
    "    for _, row in hits.iterrows()\n",
    "]\n",
    "\n",
    "#######################################################\n",
    "def build_answer(hits):\n",
    "    answer = \"Here are the relevant details from the documents:\\n\\n\"\n",
    "    for idx, hit in enumerate(hits, start=1):\n",
    "        answer += (\n",
    "            f\"{idx}. File: {hit.filename}, Page: {hit.page}, Chunk: {hit.chunk_id}\\n\"\n",
    "            f\"Content: {hit.content}\\n\\n\"\n",
    "        )\n",
    "    return answer\n",
    "\n",
    "nl_answer = build_answer(hit_objects)\n",
    "\n",
    "#print(type(nl_answer))\n",
    "print(nl_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c6b6eb6-21f1-4e70-9c45-67bae22dd2d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Here are the relevant details from the documents:\n",
    "\n",
    "1. File: recipes.pdf, Page: 12, Chunk: 3\n",
    "Content: To make a spicy chicken curry, start with...\n",
    "\n",
    "2. File: cooking_tips.pdf, Page: 7, Chunk: 1\n",
    "Content: The best way to balance heat and flavor in your curry is..."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6798478252868818,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Documentation_RAG",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
